{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish Detection\n",
    "This notebook is taken from [pytorch.org](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html).property\n",
    "\n",
    "License: BSD\n",
    "Author: Sasank Chilamkurthy\n",
    "\n",
    "## Dependencies\n",
    "torch torchvision numpy matplotlib ipykernel opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7687e8dd7310>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference:\n",
    "# Each directory in \"$DATASET_DIR\" has an \"annotations.xml\" and an \"images\"\n",
    "# directory. \"annotations.xml\" contains all annotations, while \"images\" contains\n",
    "# every frame in the video.\n",
    "#\n",
    "# To get all images in a direcotry: os.listdir(). The length of this list is\n",
    "# the total frames in the video.\n",
    "\n",
    "DATASET_DIR = \"/home/khai/mounted_drives/documents/computer_vision/fish_classification_data/annotated_videos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on Single Sample\n",
    "Let's create a test on a single image. We create a bounding box around the fish\n",
    "per annotation and label it \"fish\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_name = \"frame_000070.PNG\"\n",
    "# xml_name = \"annotations.xml\"\n",
    "\n",
    "# # Put together image\n",
    "# img = cv2.imread(os.path.join(DATASET_DIR, image_name))\n",
    "# cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert BGR to RGB\n",
    "\n",
    "# # Put together annotations\n",
    "# tree = ET.parse(os.path.join(ANNOTATIONS_DIR, xml_name))\n",
    "# root = tree.getroot()\n",
    "\n",
    "# annotation_color = (0, 255, 0)\n",
    "\n",
    "# # Extract bounding boxes from the annoations (if any)\n",
    "# # <box frame=\"70\" outside=\"0\" occluded=\"0\" keyframe=\"0\" xtl=\"327.46\" ytl=\"224.60\" xbr=\"623.15\" ybr=\"377.17\" z_order=\"0\">\n",
    "# box_frames_collection = root.find(\"track\")\n",
    "# assert box_frames_collection is not None, \"Cannot find the 'track' tag in the annotations\"\n",
    "\n",
    "# box_frames = list()\n",
    "\n",
    "# # Extract all annoation datas for each frame\n",
    "# for element in box_frames_collection:\n",
    "#     xtl = int(float(element.attrib.get('xtl')))\n",
    "#     ytl = int(float(element.attrib.get('ytl')))\n",
    "#     xbr = int(float(element.attrib.get('xbr')))\n",
    "#     ybr = int(float(element.attrib.get('ybr')))\n",
    "#     box_frames.append({\n",
    "#         \"frame\": element.attrib.get('frame'),\n",
    "#         \"x\": xtl,\n",
    "#         \"width\": xbr - xtl,\n",
    "#         \"y\": ytl,\n",
    "#         \"height\": ybr - ytl,\n",
    "#     })\n",
    "\n",
    "# fbox = dict()\n",
    "# for e in box_frames:\n",
    "#     if e.get(\"frame\") == \"70\":\n",
    "#         fbox = e\n",
    "#         break\n",
    "\n",
    "\n",
    "# # BGR colour space\n",
    "# cv2.rectangle(img,\n",
    "#             (fbox.get(\"x\"), fbox.get(\"y\")),\n",
    "#             (fbox.get(\"x\") + fbox.get(\"width\"), fbox.get(\"y\") + fbox.get(\"height\")),\n",
    "#             annotation_color,\n",
    "#             2)\n",
    "\n",
    "# # Annotate the box\n",
    "# text_position = (fbox.get(\"x\"), fbox.get(\"y\") + 20)\n",
    "# text_size = 1.5\n",
    "# text_thickness = 3\n",
    "# cv2.putText(img, \"fish\", text_position, cv2.FONT_HERSHEY_SIMPLEX, text_size, annotation_color, thickness=text_thickness)\n",
    "\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Video Clip\n",
    "This time, we will use a video clip to create a bounding box around the fish and\n",
    "also label it. We'll convert the above cell to a function.\n",
    "\n",
    "Credit to [YingqiangGao](https://stackoverflow.com/users/7759152/yingqiang-gao) at [StackOverflow](https://stackoverflow.com/q/43048725) for the code with writing to video.\n",
    "\n",
    "## Video Specifications\n",
    "FPS: 25 (Based eye judgement; 24 is too slow and 30 is too quick)\n",
    "Dimensions: 1920x1080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def label_frames(images_dir: str, annotations: str, output_dir: str) -> None:\n",
    "    try:\n",
    "        # Create output directory\n",
    "        output_images_dir = os.path.join(os.getcwd(), output_dir)\n",
    "        if not os.path.exists(output_images_dir):\n",
    "            os.mkdir(output_images_dir)\n",
    "        \n",
    "        # Create bounding boxes\n",
    "        tree = ET.parse(annotations)\n",
    "        box_frames_collection = tree.getroot().find(\"track\")\n",
    "\n",
    "        if box_frames_collection is None:\n",
    "            print(f\"Video {images_dir} does not contain annotations\")\n",
    "            return\n",
    "        \n",
    "        # Set bounding box and label colour\n",
    "        annotation_color = (0, 255, 0)\n",
    "\n",
    "        bframes = dict() # List of all boxes\n",
    "\n",
    "        # Extract all annoation datas for each frame\n",
    "        # TODO Support multple bounding boxes\n",
    "        for element in box_frames_collection:\n",
    "            xtl = int(float(element.attrib.get('xtl')))\n",
    "            ytl = int(float(element.attrib.get('ytl')))\n",
    "            xbr = int(float(element.attrib.get('xbr')))\n",
    "            ybr = int(float(element.attrib.get('ybr')))\n",
    "            bframes[element.attrib.get('frame')] = {\n",
    "                \"x\": xtl,\n",
    "                \"width\": xbr - xtl,\n",
    "                \"y\": ytl,\n",
    "                \"height\": ybr - ytl,\n",
    "            }\n",
    "\n",
    "        # Labeling each frame in the images directory\n",
    "        all_frames = os.listdir(images_dir)\n",
    "        all_frames.sort()\n",
    "        for i, image_name in enumerate(all_frames):\n",
    "            frame_num = str(i)\n",
    "\n",
    "            # Print loading bar\n",
    "            print(f\"\\tFrame {i + 1} of {len(all_frames)} \", end='|')\n",
    "            # Number of signs to print for loading bar\n",
    "            loading_bar = ((i + 1) / len(all_frames)) * 100\n",
    "            for j in range(int(loading_bar)):\n",
    "                print(\"=\", end='')\n",
    "            # Print empty signs for remaining frames to render\n",
    "            for j in range(100 - int(loading_bar)):\n",
    "                print(\" \", end='')\n",
    "            print(f\"| {100 * (i + 1) / len(all_frames):.1f}%\", end='\\r')\n",
    "\n",
    "            # Get image\n",
    "            img = cv2.imread(os.path.join(images_dir, image_name))\n",
    "            cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert BGR to RGB\n",
    "\n",
    "            if bframes.get(frame_num) is not None:  # There is a bounding box!\n",
    "                # Create bounding box\n",
    "                cv2.rectangle(img,\n",
    "                            (bframes.get(frame_num).get(\"x\"), bframes.get(frame_num).get(\"y\")),\n",
    "                            (bframes.get(frame_num).get(\"x\") + bframes.get(frame_num).get(\"width\"), bframes.get(frame_num).get(\"y\") + bframes.get(frame_num).get(\"height\")),\n",
    "                            annotation_color,\n",
    "                            2)\n",
    "\n",
    "                # Annotate the box\n",
    "                text_position = (bframes.get(frame_num).get(\"x\"), bframes.get(frame_num).get(\"y\") + 20)\n",
    "                text_size = 1.5\n",
    "                text_thickness = 3\n",
    "                cv2.putText(img, \"fish\", text_position, cv2.FONT_HERSHEY_SIMPLEX, text_size, annotation_color, thickness=text_thickness)\n",
    "                cv2.imwrite(os.path.join(output_images_dir, image_name), img)\n",
    "            else:   # No bounding box\n",
    "                # We simply copy over the file. This speeds up the processing time\n",
    "                shutil.copy(os.path.join(images_dir, image_name), os.path.join(output_images_dir, image_name))\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        raise KeyboardInterrupt(\"You have cancelled the process\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    all_videos_list = os.listdir(DATASET_DIR)\n",
    "    all_videos_list.sort()\n",
    "\n",
    "    # Get width and height\n",
    "    img_size = (1920, 1080)\n",
    "\n",
    "    print(\"This will take some time. Press CTRL+C to cancel gracefully\")\n",
    "\n",
    "    if not os.path.exists(\"/home/khai/mounted_drives/documents/computer_vision/fish_classification_data/output\"):\n",
    "        os.mkdir(\"/home/khai/mounted_drives/documents/computer_vision/fish_classification_data/output\")\n",
    "\n",
    "    for i, img in enumerate(all_videos_list):\n",
    "        print(f\"Labelling video {i + 1} of {len(all_videos_list)}\")\n",
    "\n",
    "        label_frames(\n",
    "            images_dir=os.path.join(DATASET_DIR, img, \"images\"),\n",
    "            annotations=os.path.join(DATASET_DIR, img, \"annotations.xml\"),\n",
    "            output_dir=str(f\"/home/khai/mounted_drives/documents/computer_vision/fish_classification_data/output/{i}.{img}\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CNN Model to Classify the Fish\n",
    "- Specifications: 80% training, 20% testing\n",
    "\n",
    "The first 80% of the dataset is for training, and the last 20% for testing. The\n",
    "order is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference: all our dataset is in \"$DATASET_DIR\"\n",
    "import random\n",
    "\n",
    "\n",
    "all_dataset = os.listdir(DATASET_DIR)\n",
    "\n",
    "# Randomize the list\n",
    "random.seed(42)\n",
    "random.shuffle(all_dataset)\n",
    "\n",
    "trainset = all_dataset[0: int(len(all_dataset) * 0.8)]\n",
    "testset = all_dataset[int(len(all_dataset) * 0.8):]\n",
    "classes = ['not_fish', 'fish']\n",
    "class_labels = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Convolutional Neural Network\n",
    "In this example, we will implement the following:\n",
    "\n",
    "- Convolution: A 2d convolutional layer with `x` input channels, `y` output channels and kernel size of `z * z`\n",
    "- Max pooling: A layer with `x * x` kernel size and stride of `y`\n",
    "- Linear layer: A fully connected layer with `x` input features and `y` output features. `nn.Linear(16*5*5, 120)` means 16 features, 5x5 spatial dimensions and output size of 120.\n",
    "\n",
    "## Max Pooling\n",
    "A technique to downsample data,, commonly used in convolutional neural networks. It divides areas in the data into non-overlapping blocks and picks the maximum value in each block.\n",
    "\n",
    "If you want to know more, use your favourite search engine to look them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convolutional neural network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolutional layer: applies a filter to the input image to extract features.\n",
    "        self.conv1 = nn.Conv2d(3,6,5)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # ReLU: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "    # Propagate one step through the network.\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['task_fish_detection-2024_01_28_18_22_01-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_41_54-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_18_34-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_29_12-cvat_for_video_1.1', 'task_fish_detection-2024_02_02_06_10_27-cvat_for_video_1.1', 'task_fish_detection-2024_02_01_18_51_52-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_33_21-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_28_06-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_12_47-cvat_for-video-1.1', 'task_fish_detection-2024_01_28_18_55_01-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_19_00_18-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_51_06-cvat_for_video_1.1', 'task_fish_detection-2024_02_01_18_41_23-cvat_for_video_1.1', 'task_fish_detection-2024_02_01_18_27_02-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_30_27-cvat_for_video_1.1', 'task_fish_detection-2024_02_01_18_34_18-cvat_for_video_1.1', 'task_fish_detection-2024_02_01_18_51_23-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_44_28-cvat_for_video_1.1', 'task_fish_detection-2024_02_01_18_55_41-cvat_for_video_1.1', 'task_fish_detection-2024_01_28_18_07_53-cvat_for_video_1.1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['images', 'annotations.xml']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainset)\n",
    "os.listdir(os.path.join(DATASET_DIR, trainset[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# Cross entropy loss: Measure the difference between predicted probability and\n",
    "# the truth. There are many loss functions, but cross entropy is commonly used\n",
    "# in classification problems.\n",
    "# In short, loss is a quantitative measure of how our model's prediction\n",
    "# performs.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Stochastic gradient descent: Minimize the loss function.\n",
    "# Learning rate: Step size in learning process. Too high value: overshoots the optimal value. Too low: too slow process.\n",
    "# Momentum: Adds a fraction from the vector in the previous time step to the current.\n",
    "#\n",
    "# For info: an optimiser aims to improve the model's predictions (performance)\n",
    "# by adjusting weights. Weights impacts how much a node's output contributes to\n",
    "# the propagated node's input.\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2\n",
      "[1,    10] loss: 0.012\n",
      "[1,    20] loss: 0.012\n",
      "[1,    30] loss: 0.012\n",
      "[1,    40] loss: 0.012\n",
      "[1,    50] loss: 0.012\n",
      "[1,    60] loss: 0.012\n",
      "[1,    70] loss: 0.012\n",
      "[1,    80] loss: 0.012\n",
      "[1,    90] loss: 0.012\n",
      "[1,   100] loss: 0.012\n",
      "[1,   110] loss: 0.012\n",
      "[1,   120] loss: 0.012\n",
      "[1,   130] loss: 0.012\n",
      "[1,   140] loss: 0.012\n",
      "[1,   150] loss: 0.012\n",
      "[1,   160] loss: 0.012\n",
      "Frame 164 of 164\n",
      "Epoch 2 of 2\n",
      "[2,    10] loss: 0.012\n",
      "[2,    20] loss: 0.012\n",
      "[2,    30] loss: 0.012\n",
      "[2,    40] loss: 0.012\n",
      "[2,    50] loss: 0.012\n",
      "[2,    60] loss: 0.012\n",
      "[2,    70] loss: 0.012\n",
      "[2,    80] loss: 0.012\n",
      "[2,    90] loss: 0.012\n",
      "[2,   100] loss: 0.012\n",
      "[2,   110] loss: 0.012\n",
      "[2,   120] loss: 0.012\n",
      "[2,   130] loss: 0.012\n",
      "[2,   140] loss: 0.012\n",
      "[2,   150] loss: 0.012\n",
      "[2,   160] loss: 0.012\n",
      "Frame 164 of 164\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Add transformers to our model\n",
    "transform = transforms.Compose ([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Train the network\n",
    "# Please note this takes time, depending on your hardware. At the moment, we use\n",
    "# CPU. This is slow!\n",
    "# If you have saved the model in a file, this step is only necessary to\n",
    "# execute when you want to improve or make changes to the model.\n",
    "\n",
    "# Let's try to train only for one of the videos\n",
    "\n",
    "video = trainset[10]\n",
    "\n",
    "video_path = os.path.join(DATASET_DIR, video, \"images\")\n",
    "\n",
    "\n",
    "# Get all annotations\n",
    "annotations = os.path.join(DATASET_DIR, video, \"annotations.xml\")\n",
    "labels = [\"no-fish\"] * int(len(video_path))  # List of frames with visible fish\n",
    "if os.path.exists(annotations):\n",
    "    tree = ET.parse(annotations)\n",
    "    box_frames_collection = tree.getroot().find(\"track\")\n",
    "\n",
    "    if box_frames_collection is None:\n",
    "        raise TypeError(f\"Video {video} is not annotated\")\n",
    "\n",
    "    for element in box_frames_collection:\n",
    "        labels[int(element.attrib.get('frame'))] = \"fish\"\n",
    "\n",
    "\n",
    "frames = os.listdir(video_path)\n",
    "frames.sort()\n",
    "\n",
    "for epoch in range(2): # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    print(f\"Epoch {epoch+1} of {2}\")\n",
    "\n",
    "    for i, data in enumerate(frames, 0):\n",
    "        print(f\"Frame {i+1} of {len(frames)}\", end='\\r')\n",
    "        img = Image.open(os.path.join(video_path, data))\n",
    "        img.thumbnail((30, 30), Image.Resampling.LANCZOS)\n",
    "        # get the inputs from dataset\n",
    "        # inputs = image\n",
    "        # labels = class\n",
    "        # data = data from the trainloader\n",
    "        inputs = transform(img)\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        inputs = inputs.repeat(4,1,1,1)\n",
    "        # print(inputs.shape)\n",
    "        # break\n",
    "\n",
    "        # If there is an annotation for the current frame, then label is \"fish\".\n",
    "        # Otherwise it's \"no_fish\"\n",
    "\n",
    "\n",
    "        # zero the parameter gradients. Gradients means the change in the\n",
    "        # weights\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward propagation + optimize\n",
    "        # This is how we traverse the neural network. In CNN we perform forward\n",
    "        # and backward propagation before we optimize the weights.\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        c_labels = torch.LongTensor(class_labels).repeat(2)\n",
    "        \n",
    "        loss = criterion(outputs, c_labels)\n",
    "        # loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9: # print every n mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
